{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cfbdf5dc10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (28,28)\n",
    "mean = [0.485, 0.456, 0.406] \n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),  # Resize to a fixed size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Iterate over each subfolder (assuming one for dogs and one for cats)\n",
    "        for label, folder_name in enumerate(['dog', 'cat']):\n",
    "            folder_path = os.path.join(self.root_dir, folder_name)\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Try opening the file with PIL to check if it's a valid image\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        \n",
    "                        if img.mode != 'RGB':\n",
    "                            print(f\"Skipping {file_path} because it does not have 3 channels (RGB)\")\n",
    "                            continue\n",
    "\n",
    "                        self.image_paths.append(file_path)\n",
    "                        self.labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping {file_path} due to error: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        with Image.open(image_path) as img:\n",
    "\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\10158.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\10401.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\10747.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\10797.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\11285.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\11410.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\11675.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\11702.jpg due to error: cannot identify image file 'D:\\\\Embedded ML\\\\QAT-MobileNetV2\\\\PetImages\\\\dog\\\\11702.jpg'\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\11849.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\11853.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\1259.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\1308.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\1773.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\1789.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\1866.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\2384.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\2688.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\2877.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\3136.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\3288.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\3588.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\3823.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\4367.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\5604.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\5736.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\6059.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\6238.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\6245.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\6318.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\6718.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\7112.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\7133.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\7369.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\7459.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\7514.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\7969.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\8730.jpg because it does not have 3 channels (RGB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python39\\lib\\site-packages\\PIL\\TiffImagePlugin.py:845: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\9078.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\9188.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\dog\\Thumbs.db due to error: cannot identify image file 'D:\\\\Embedded ML\\\\QAT-MobileNetV2\\\\PetImages\\\\dog\\\\Thumbs.db'\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\10125.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\10501.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\10820.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\11095.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\11210.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\11565.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\11874.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\11935.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\12080.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\140.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\2663.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\2939.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\3300.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\3491.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\4833.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\5370.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\5553.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\5686.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\6435.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\660.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\666.jpg due to error: cannot identify image file 'D:\\\\Embedded ML\\\\QAT-MobileNetV2\\\\PetImages\\\\cat\\\\666.jpg'\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\7276.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\7968.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\7978.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\8470.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\850.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\9171.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\936.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\9565.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\9778.jpg because it does not have 3 channels (RGB)\n",
      "Skipping D:\\Embedded ML\\QAT-MobileNetV2\\PetImages\\cat\\Thumbs.db due to error: cannot identify image file 'D:\\\\Embedded ML\\\\QAT-MobileNetV2\\\\PetImages\\\\cat\\\\Thumbs.db'\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(root_dir='D:\\Embedded ML\\QAT-MobileNetV2\\PetImages', transform=transform)\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, data_loader, device,epoch):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    for batch_idx, (image, target) in enumerate(data_loader):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(image)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Print batch loss (optional)\n",
    "        # print(f\"Batch [{batch_idx + 1}/{num_batches}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch = {epoch+1} || Training Loss: {avg_epoch_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, data_loader, device,epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "       \n",
    "        for image, target in data_loader:\n",
    "            image, target = image.to(device), target.to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            # Accumulate batch loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output, 1)  # Get the predicted class index\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            total_predictions += target.size(0)\n",
    "            \n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"Epoch = {epoch+1} || Test Loss: {avg_epoch_loss:.4f} || Test Accuracy: {accuracy:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.model = models.mobilenet_v2(weights='MobileNet_V2_Weights.DEFAULT')  \n",
    "        \n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        self.model.classifier[1] = nn.Sequential(\n",
    "            nn.Linear(in_features=self.model.classifier[1].in_features,out_features=512),\n",
    "            nn.LeakyReLU(negative_slope=0.02,inplace=False),\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.Dropout(p=0.4,inplace=False),\n",
    "            nn.Linear(in_features=512,out_features=2),\n",
    "            nn.Softmax(dim=1))\n",
    "        \n",
    "        # print(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.76 MB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "chip = platform.processor()\n",
    "\n",
    "# if chip == 'arm':\n",
    "#     backend = 'qnnpack'\n",
    "# elif chip in ['x86_64', 'i386']:\n",
    "#     backend = 'fbgemm'\n",
    "# else:\n",
    "#     raise SystemError(\"Backend is not supported\")\n",
    "\n",
    "# print(f\"Using {backend} backend engine for {chip} CPU\")\n",
    "\n",
    "backend = 'x86'\n",
    "\n",
    "torch.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushovan.saha\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\ao\\quantization\\fx\\prepare.py:1751: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sushovan.saha\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\ao\\quantization\\observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sushovan.saha\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\ao\\quantization\\fx\\utils.py:857: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x000001CFC4BD6B80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x000001CFC4BD6B80>}).\n",
      "Please use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n",
      "    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
      "    model = prepare_fx(model, qconfig_mapping, example_inputs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx,prepare_qat_fx\n",
    "\n",
    "example_inputs = (torch.randn(1, 3, 28, 28),)\n",
    "qconfig = {\n",
    "    \"\": torch.quantization.get_default_qat_qconfig(backend),\n",
    "    \"module_name\": [\n",
    "    #    (\"features.13\", None),    \n",
    "    #    (\"features.14\", None),\n",
    "    #    (\"features.15\", None),\n",
    "    #    (\"features.16\", None),\n",
    "    #    (\"features.17\", None),\n",
    "    ]\n",
    "}\n",
    "#model_prepared = prepare_fx(model.eval(), qconfig,example_inputs)\n",
    "model_prepared = prepare_qat_fx(model.train(), qconfig, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.AdamW(model_prepared.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python39\\lib\\site-packages\\PIL\\TiffImagePlugin.py:845: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 || Training Loss: 0.6354\n",
      "Epoch = 1 || Test Loss: 0.5938 || Test Accuracy: 0.6952\n",
      "Epoch = 2 || Training Loss: 0.5872\n",
      "Epoch = 2 || Test Loss: 0.5731 || Test Accuracy: 0.7175\n",
      "Epoch = 3 || Training Loss: 0.5638\n",
      "Epoch = 3 || Test Loss: 0.5601 || Test Accuracy: 0.7345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     model_quantized \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model_prepared)\n\u001b[0;32m      4\u001b[0m     model_quantized \u001b[38;5;241m=\u001b[39m convert_fx(model_quantized\u001b[38;5;241m.\u001b[39meval())\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, criterion, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m image, target \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\fx\\graph_module.py:738\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\fx\\graph_module.py:304\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[1;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls, obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m<eval_with_key>.2:206\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    204\u001b[0m model_classifier_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mclassifier, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)(activation_post_process_99);  activation_post_process_99 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    205\u001b[0m activation_post_process_100 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process_100(model_classifier_0);  model_classifier_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m model_classifier_1_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_post_process_100\u001b[49m\u001b[43m)\u001b[49m;  activation_post_process_100 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    207\u001b[0m activation_post_process_101 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process_101(model_classifier_1_0);  model_classifier_1_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    208\u001b[0m model_classifier_1_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mclassifier, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)(activation_post_process_101);  activation_post_process_101 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\ao\\nn\\qat\\modules\\linear.py:41\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_fake_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for nepoch in range(20):\n",
    "    train_epoch(model_prepared, criterion, optimizer, train_loader, torch.device('cpu'),nepoch)\n",
    "    model_quantized = copy.deepcopy(model_prepared)\n",
    "    model_quantized = convert_fx(model_quantized.eval())\n",
    "    evaluate(model_quantized,criterion, test_loader,torch.device('cpu'),nepoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 || Test Loss: 0.5069 || Test Accuracy: 0.7930\n"
     ]
    }
   ],
   "source": [
    "model_quantized = copy.deepcopy(model_prepared)\n",
    "model_quantized = convert_fx(model_quantized.eval())\n",
    "evaluate(model_quantized,criterion, data_loader_test,torch.device('cpu'),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.30 MB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_net = torch.jit.trace(model_quantized, torch.randn(1,3,28,28))\n",
    "\n",
    "torch.jit.save(traced_net,'QATDogCatMobileNetV2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
